#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æŠ“å–å½’æ¡£å·¥å…·
WeChat Article Archive Tool
"""

import requests
from bs4 import BeautifulSoup
import time
import random
import re
import json
import hashlib
import sys
from datetime import datetime
from pathlib import Path


def fetch_wechat_article(url, delay_range=(1, 3), timeout=10):
    """
    æŠ“å–å¾®ä¿¡å…¬ä¼—å·æ–‡ç« å†…å®¹

    Args:
        url: å¾®ä¿¡å…¬ä¼—å·æ–‡ç« å®Œæ•´URL
        delay_range: éšæœºå»¶è¿ŸèŒƒå›´ï¼ˆç§’ï¼‰
        timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

    Returns:
        dict: æŠ“å–ç»“æœï¼Œå¤±è´¥æ—¶åŒ…å« error å­—æ®µ
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
    }

    try:
        # éšæœºå»¶è¿Ÿï¼Œé™ä½è¢«é£æ§æ¦‚ç‡
        time.sleep(random.uniform(*delay_range))

        print("ğŸš€ æ­£åœ¨æŠ“å–æ–‡ç« ...")
        resp = requests.get(url, headers=headers, timeout=timeout)
        resp.raise_for_status()
        resp.encoding = 'utf-8'

        soup = BeautifulSoup(resp.text, 'html.parser')

        # æå–æ ‡é¢˜
        title_tag = soup.find('h1', class_='rich_media_title')
        title = title_tag.get_text(strip=True) if title_tag else "æœªçŸ¥æ ‡é¢˜"

        # æå–æ­£æ–‡
        content_div = soup.find('div', class_='rich_media_content')
        content = ""
        if content_div:
            for elem in content_div.select('p, section, h1, h2, h3, h4'):
                text = elem.get_text(strip=True)
                if text:
                    # ä¿æŒæ ‡é¢˜æ ¼å¼
                    if elem.name in ['h1', 'h2', 'h3', 'h4']:
                        content += f"\n## {text}\n\n"
                    else:
                        content += f"{text}\n\n"
            content = content.strip()

        # æå–ä½œè€…
        author_tag = soup.find('span', class_='rich_media_meta rich_media_meta_text')
        author = author_tag.get_text(strip=True) if author_tag else ""

        # æå–å‘å¸ƒæ—¶é—´
        publish_time = "æœªçŸ¥æ—¶é—´"

        # æ–¹æ³•1: ä»JSå˜é‡æå–
        time_match = re.search(r'var ct\s*=\s*"(\d+)"', resp.text)
        if time_match:
            ts = int(time_match.group(1))
            publish_time = datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')
        else:
            # æ–¹æ³•2: ä»metaæ ‡ç­¾æå–
            meta_time = soup.find('meta', {'property': 'og:article:published_time'}) or \
                       soup.find('meta', {'property': 'article:published_time'}) or \
                       soup.find('em', id='publish_time')
            if meta_time:
                content_time = meta_time.get('content') if meta_time.name == 'meta' else meta_time.get_text(strip=True)
                if content_time:
                    publish_time = content_time

        print(f"âœ… æˆåŠŸæŠ“å–: {title}")

        return {
            "title": title,
            "content": content,
            "author": author,
            "publish_time": publish_time,
            "url": url,
            "status": "success"
        }

    except Exception as e:
        return {
            "error": f"æŠ“å–å¤±è´¥: {str(e)}",
            "url": url,
            "status": "failed"
        }


class WeChatArchiveManager:
    """å¾®ä¿¡æ–‡ç« å½’æ¡£ç®¡ç†å™¨"""

    def __init__(self, vault_dir="outputs/20-é˜…è¯»ç¬”è®°"):
        self.vault_dir = Path(vault_dir)
        self.vault_dir.mkdir(parents=True, exist_ok=True)

    def generate_slug(self, title, url_hash):
        """ç”Ÿæˆæ–‡ä»¶å¤¹åç§°"""
        date_str = datetime.now().strftime("%Y%m%d")
        clean_title = re.sub(r'[^\w\s-]', '', title)
        clean_title = re.sub(r'[-\s]+', '-', clean_title.strip())[:30]
        return f"{date_str}-{clean_title}-{url_hash[:6]}"

    def create_obsidian_note(self, result, folder_path):
        """åˆ›å»º Obsidian æ ¼å¼çš„ç¬”è®°"""
        slug = folder_path.name
        filename = f"{slug}.md"
        note_path = folder_path / filename

        # åˆ›å»º front matter
        front_matter = {
            "title": result['title'],
            "author": result.get('author', ''),
            "date": result.get('publish_time', datetime.now().strftime('%Y-%m-%d')),
            "url": result['url'],
            "tags": ["wechat", "article"],
            "archived": datetime.now().isoformat()
        }

        # åˆ›å»ºç¬”è®°å†…å®¹
        note_content = f"""---
{json.dumps(front_matter, ensure_ascii=False, indent=2)}
---

# {result['title']}

## ğŸ“– æ–‡ç« ä¿¡æ¯

- **ä½œè€…**: {result.get('author', 'æœªçŸ¥')}
- **å‘å¸ƒæ—¶é—´**: {result.get('publish_time', 'æœªçŸ¥')}
- **åŸæ–‡é“¾æ¥**: [ğŸ”— ç‚¹å‡»é˜…è¯»]({result['url']})

## ğŸ” å†…å®¹æ‘˜è¦

> æ ¸å¿ƒæ‘˜è¦å†…å®¹...

## ğŸ’¡ æ ¸å¿ƒè§‚ç‚¹

1.
2.
3.

## ğŸ“š å…³é”®æ¦‚å¿µ

1. **æ¦‚å¿µ1**:
2. **æ¦‚å¿µ2**:
3. **æ¦‚å¿µ3**:

## ğŸ¤” ä¸ªäººæ€è€ƒ

-
-

## ğŸ“‹ è¡ŒåŠ¨é¡¹

- [ ] æ•´ç†ç¬”è®°
- [ ] å»¶ä¼¸é˜…è¯»
- [ ] å®è·µåº”ç”¨

---
*å½’æ¡£äº: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}*
"""

        with open(note_path, 'w', encoding='utf-8') as f:
            f.write(note_content)
        return note_path

    def save_original_article(self, result, folder_path):
        """ä¿å­˜åŸå§‹æ–‡ç« """
        content = result.get('content', '')

        # å¦‚æœå†…å®¹æ²¡æœ‰æ ‡é¢˜ï¼Œæ·»åŠ æ ‡é¢˜
        if not content.startswith('#'):
            content = f"# {result['title']}\n\n{content}"

        article_content = f"""{content}

---
*åŸæ–‡å‘å¸ƒ: {result.get('publish_time', '')}*
*åŸæ–‡é“¾æ¥: {result['url']}*
"""

        article_path = folder_path / "article.md"
        with open(article_path, 'w', encoding='utf-8') as f:
            f.write(article_content)
        return article_path

    def save_metadata(self, result):
        """ä¿å­˜å…ƒæ•°æ®"""
        meta = {
            "title": result['title'],
            "author": result.get('author', ''),
            "publish_time": result.get('publish_time', ''),
            "url": result['url'],
            "archived_at": datetime.now().isoformat(),
            "word_count": len(result.get('content', '').split()),
            "content_hash": hashlib.sha256(result.get('content', '').encode()).hexdigest()
        }
        return meta

    def archive_article(self, url):
        """
        å®Œæ•´å½’æ¡£æµç¨‹

        Args:
            url: å¾®ä¿¡å…¬ä¼—å·æ–‡ç« URL

        Returns:
            dict: å½’æ¡£ç»“æœ
        """
        print(f"\nå¼€å§‹å½’æ¡£: {url}")

        # 1. æŠ“å–æ–‡ç« 
        result = fetch_wechat_article(url)

        if result['status'] == 'failed':
            return result

        # 2. ç”Ÿæˆè·¯å¾„
        url_hash = hashlib.sha256(url.encode()).hexdigest()
        slug = self.generate_slug(result['title'], url_hash)

        folder_path = self.vault_dir / slug
        folder_path.mkdir(exist_ok=True)

        print("ğŸ“ æ­£åœ¨ä¿å­˜æ–‡ä»¶...")

        # 3. ä¿å­˜æ–‡ä»¶
        note_path = self.create_obsidian_note(result, folder_path)
        article_path = self.save_original_article(result, folder_path)

        # 4. ä¿å­˜å…ƒæ•°æ®
        meta = self.save_metadata(result)
        meta_path = folder_path / "meta.json"
        with open(meta_path, 'w', encoding='utf-8') as f:
            json.dump(meta, f, ensure_ascii=False, indent=2)

        print("âœ¨ å½’æ¡£å®Œæˆ!")
        print(f"ğŸ“ æ–‡ä»¶ä½ç½®: {folder_path}")

        return {
            "status": "success",
            "title": result['title'],
            "author": result.get('author', ''),
            "publish_time": result.get('publish_time', ''),
            "folder": str(folder_path),
            "slug": slug,
            "word_count": len(result.get('content', '').split())
        }